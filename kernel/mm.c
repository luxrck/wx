#include <x86.h>
#include <string.h>
#include <errno.h>

#include <kernel/mm.h>
#include <kernel/proc.h>
#include <kernel/kclock.h>

extern char bootstacktop[], bootstack[];

static void* boot_alloc(uint32_t n)
{
	static char *nextfree;	// virtual address of next byte of free memory
	char *result;

	// Initialize nextfree if this is the first time.
	// 'end' is a magic symbol automatically generated by the linker,
	// which points to the end of the kernel's bss segment:
	// the first virtual address that the linker did *not* assign
	// to any kernel code or global variables.
	if (!nextfree) {
		extern char end[];
		nextfree = ROUNDUP((char *) end, PGSIZE);
	}

	// Allocate a chunk large enough to hold 'n' bytes, then update
	// nextfree.  Make sure nextfree is kept aligned
	// to a multiple of PGSIZE.
	//
	// LAB 2: Your code here.
	if (n > npages *PGSIZE) panic("Out of memory.");
	char *page = nextfree;
	if (n) nextfree = ROUNDUP(nextfree + n, PGSIZE);
	return page;
}

void meminit(void)
{
	// Use CMOS calls to measure available base & extended memory.
	// Our memory is always bigger than 1MB.
	//
	// 017-018 - Low and High Expansion Memory Bytes
	// These bytes define the amount of memory above the 1 Mb address space.
	// The value from these bytes represents the number of 1 Kb blocks of
	// expansion memory. For example 800h is equal to 2048Kb.
	// http://www.ousob.com/ng/interrupts_and_ports/ng931a1.php
	npages = ((cmos_read(0x17) | (cmos_read(0x18)<<8)) * 1024) / PGSIZE + 0x100;

	uint32_t cr0;
	size_t i, n;
	struct pages *page = NULL;
	physaddr_t pa;
	pte_t *pte = NULL;

	kpgdir = (pde_t *) boot_alloc(PGSIZE);
	memset(kpgdir, 0, PGSIZE);

	pages = boot_alloc(npages * sizeof(struct page));
	memset(pages, 0, npages * sizeof(struct page));
	pagebase = pages;

	procs = boot_alloc(NPROC * sizeof(struct proc));
	memset(procs, 0, NPROC * sizeof(struct proc));

	pageinit();

	varemap(kpgdir, (uintptr_t)pages, npages * sizeof(struct page), PADDR(pages), PTE_W);
	varemap(kpgdir, (uintptr_t)procs, NPROC * sizeof(struct proc), PADDR(procs), PTE_W);
	varemap(kpgdir, KSTACKTOP-KSTKSIZE, KSTKSIZE, PADDR(bootstack), PTE_W);
	varemap(kpgdir, KERNBASE, 0xFFFFFFFF - KERNBASE + 1, 0, PTE_W);

	// Switch from the minimal entry page directory to the full kpgdir
	// page table we just created.	Our instruction pointer should be
	// somewhere between KERNBASE and KERNBASE+4MB right now, which is
	// mapped the same way by both page tables.
	//
	// If the machine reboots at this point, you've probably set up your
	// kpgdir wrong.
	lcr3(PADDR(kpgdir));

	// entry.S set the really important flags in cr0 (including enabling
	// paging).  Here we configure the rest of the flags that we care about.
	cr0 = rcr0();
	cr0 |= CR0_PE|CR0_PG|CR0_AM|CR0_WP|CR0_NE|CR0_MP;
	cr0 &= ~(CR0_TS|CR0_EM);
	lcr0(cr0);
}

void pageinit(void)
{
	size_t kpe = (size_t)boot_alloc(0) - KERNBASE;
	struct page *cpi = pages;
	for (size_t i = 1; i < npages; i++) {
		size_t addr = i * PGSIZE;
		if (addr < IOPHYSMEM || addr >= kpe) {
			cpi->next = &pages[i];
			cpi = cpi->next;
		} else if (addr >= EXTPHYSMEM)
			pages[i].ref = 1;
	}
}

struct page* pagealloc(bool fill_zero)
{
	if (!pages) return NULL;
	struct page *page = pages;
	pages = pages->next;
	page->next = NULL;
	if (fill_zero) memset(page2kva(page), 0, PGSIZE);
	return page;
}

pte_t* pteget(pde_t *pgdir, const void *va, int create)
{
	pde_t pp = pgdir[PDX(va)];
	pte_t *pte = (pte_t*)KADDR(PTE_ADDR(pp));
	if (pp) {
		pte = &pte[PTX(va)];
		if(!(create || *pte)) return NULL;
		*pte |= PTE_P;
		return pte;
	}
	struct page *pg = NULL;
	if (!create) return NULL;
	if (!(pg = pagealloc(1))) return NULL;
	pg->ref++;
	pgdir[PDX(va)] = page2pa(pg) | PTE_W | PTE_P;
	pte = (pte_t*)KADDR(PTE_ADDR(pgdir[PDX(va)]));
	return &pte[PTX(va)];
}

void pageforeach(pde_t *pgdir, int(*cb)(int index, pte_t pte, void *data), void *data)
{
	for (int i=0; i<0x3FF; i++) {
		if (!pgdir[i]) continue;
		pte_t *pte = KADDR(PTE_ADDR(pgdir[i]));
		for (int j=0; j<0x3FF; j++) {
			if (!pte[j]) continue;
			if (!cb(i*0x400+j, pte[j], data)) return;
		}
	}
}

// page map
int vamap(pde_t *pgdir, struct page *pp, void *va, int perm)
{
	vaunmap(pgdir, va);
	pp->ref++;
	physaddr_t paddr = page2pa(pp);
	pte_t *pte = pteget(pgdir, va, 1);
	if (!pte) goto err;
ret:
	*pte = paddr | perm | PTE_P;
	pgdir[PDX(va)] |= perm | PTE_P;
	return 0;
err:
	pp->ref--;
	return -E_NO_MEM;
}

struct page* pageget(pde_t *pgdir, void *va, pte_t **pte_store)
{
	pte_t *pte = NULL;
	pte = pteget(pgdir, va, 0);
	if (!pte) return NULL;
	if (pte_store)
		*pte_store = pte;
	return pa2page(PTE_ADDR(*pte) | PGOFF(va));
}

void pagedecref(struct page *pg)
{
	if (!--pg->ref) {
		pg->next = pages;
		pages = pg;
	}
}

void vaunmap(pde_t *pgdir, void *va)
{
	pte_t *pte = NULL;
	struct page *pg = pageget(pgdir, va, &pte);
	if (!pg) return;
	pagedecref(pg);
	*pteget(pgdir, va, 0) = 0;
	tlb_invalidate(pgdir, va);
}

void varemap(pde_t *pgdir, uintptr_t va, size_t size, physaddr_t pa, int perm)
{
	for (size_t i = 0; i < size; i += PGSIZE) {
		pte_t *pte = pteget(pgdir, (void *)(va+i), 1);
		if (!pte) panic("boot_map_region:pgdir_walk failed");
		*pte = (pa+i)|perm|PTE_P;
	}
}

void tlb_invalidate(pde_t *pgdir, void *va)
{
	// Flush the entry only if we're modifying the current address space.
	if (!ctask || ctask->pgdir == pgdir)
		invlpg(va);
}
